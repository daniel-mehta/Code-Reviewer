# ğŸ” Code Reviewer â€” LLM-Powered Code Review Tool
![Python](https://img.shields.io/badge/python-3.8-blue)
![Built with Jupyter](https://img.shields.io/badge/built%20with-Jupyter-orange)
![Made with VS Code](https://img.shields.io/badge/made%20with-VS%20Code-1f425f.svg)
![Ollama](https://img.shields.io/badge/LLM-Ollama-green)
![DeepSeek Coder](https://img.shields.io/badge/Model-DeepSeek--Coder-blueviolet)
![License](https://img.shields.io/badge/license-MIT-green)
![Status: MVP](https://img.shields.io/badge/status-MVP-yellow)
![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen)




A local, terminal-based code review assistant using open-source LLMs like `deepseek-coder` via [Ollama](https://ollama.com).

> ğŸ› ï¸ Built in a single afternoon as a quick way to learn and implement Ollama with pre-trained local LLMs â€” fast, focused, and functional.

---

## Features

- ğŸ§¾ Loads and analyzes `.py` or `.txt` code files
- ğŸ¤– Reviews powered by `deepseek-coder` running locally (via Ollama)
- ğŸ“ Outputs structured markdown:
  - `## Summary`
  - `## Issues`
  - `## Suggestions`
- ğŸ’¾ Optionally saves to `.md` for sharing or documentation
- ğŸ” Modular design with selectable review modes

---

## ğŸ§  Why?

Traditional linters focus on syntax and formatting. This tool provides **high-level feedback** on logic, clarity, and structure â€” in seconds.

Great for:
- Practicing clean code
- Reviewing your own scripts before a PR
- Learning what makes readable, maintainable code

---

## ğŸš€ Quickstart

### 1. Install Ollama

Follow the instructions at [ollama.com/download](https://ollama.com/download)

Then pull the model:

```bash
ollama run deepseek-coder
```

### 2. Clone this repo
```bash
git clone https://github.com/daniel-mehta/Code-Reviewer
cd Code-Reviewer
```

### 3. Run it
```bash
python reviewer.py
```
Follow the prompts to:
- Select a file
- Choose a review mode
- Save the output (or just print it)
---
## ğŸ—‚ Project Structure
```bash
code-reviewer/
â”œâ”€â”€ reviews/             # Saved markdown reviews
â”œâ”€â”€ sample_scripts/      # Test code files
â”œâ”€â”€ utils.py             # File loading, prompt building, saving
â”œâ”€â”€ reviewer.py          # LLM call, review pipeline, CLI logic
â”œâ”€â”€ prototype.ipynb      # Prototype notebook (initial dev/testing)
â””â”€â”€ README.md            # You're here
```
---
## ğŸ§ª Example Output
```bash
## Summary
Defines a recursive factorial function and a greeting function. Prints the result of both.

## Issues
- No input validation for negative values in factorial
- Lacks docstrings or clear comments
- greet() uses string concatenation instead of f-string

## Suggestions
- Add input validation for edge cases
- Use `f"hello {name}"` for readability
- Include comments/docstrings for clarity
```
---
## ğŸ§© Modes
- **default** â€” general, practical suggestions
- **beginner_friendly** â€” simplified explanations for newer devs
- **strict** â€” senior dev style, focused on maintainability and performance
---
## ğŸŒ± Future Ideas
- Batch-review folders (*.py)
- Live web frontend with file upload (WIP)
---
## ğŸ›  Built With
- Python 3.8.5
- Ollama
- deepseek-coder
- Jupyter Notebook (prototype phase)
- Visual Studio Code
---
## ğŸ“„ License
MIT â€” free to use, modify, or extend.

---
## âœï¸ Notes
This project was built entirely in an afternoon to quickly explore the power of running local LLMs with Ollama.

